{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the boto3 the get data from S3\n",
    "import boto3\n",
    "import io\n",
    "import pandas as pd\n",
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "credentials = credentials.get_frozen_credentials()\n",
    "access_key = '*******'\n",
    "secret_key = '**********'\n",
    "s3 = session.resource('s3')\n",
    "client = boto3.client('s3', aws_access_key_id=access_key,\n",
    "      aws_secret_access_key=secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PersonID</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>LastName</th>\n",
       "      <th>DOB</th>\n",
       "      <th>SSN</th>\n",
       "      <th>Gender</th>\n",
       "      <th>ZipCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42890708</td>\n",
       "      <td>Belinda</td>\n",
       "      <td>Harper</td>\n",
       "      <td>1980-05-16</td>\n",
       "      <td>563613038</td>\n",
       "      <td>F</td>\n",
       "      <td>93505.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42890709</td>\n",
       "      <td>Alberto</td>\n",
       "      <td>Torres</td>\n",
       "      <td>1964-12-12</td>\n",
       "      <td>611626678</td>\n",
       "      <td>M</td>\n",
       "      <td>90638.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42890710</td>\n",
       "      <td>Kiran</td>\n",
       "      <td>Mann</td>\n",
       "      <td>1970-08-29</td>\n",
       "      <td></td>\n",
       "      <td>F</td>\n",
       "      <td>95337.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42890712</td>\n",
       "      <td>Tazari</td>\n",
       "      <td>Woodward</td>\n",
       "      <td>1998-11-05</td>\n",
       "      <td>607114739</td>\n",
       "      <td>F</td>\n",
       "      <td>91764.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42890713</td>\n",
       "      <td>Donna</td>\n",
       "      <td>Garvey</td>\n",
       "      <td>1947-03-06</td>\n",
       "      <td>523669738</td>\n",
       "      <td>F</td>\n",
       "      <td>80014.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PersonID FirstName  LastName         DOB        SSN Gender  ZipCode\n",
       "0  42890708   Belinda    Harper  1980-05-16  563613038      F  93505.0\n",
       "1  42890709   Alberto    Torres  1964-12-12  611626678      M  90638.0\n",
       "2  42890710     Kiran      Mann  1970-08-29                 F  95337.0\n",
       "3  42890712    Tazari  Woodward  1998-11-05  607114739      F  91764.0\n",
       "4  42890713     Donna    Garvey  1947-03-06  523669738      F  80014.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to read the csv, we can use this code\n",
    "bucket = s3.Bucket('Bucket')\n",
    "s3_file_key = 'file/name.csv'\n",
    "#bucket = 'Bucket'\n",
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket=bucket.name, Key=s3_file_key)\n",
    "millimanintelli_df = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "millimanintelli_df.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "millimanintelli/\n",
      "millimanintelli/milliam_matched_48k.csv_$folder$\n",
      "millimanintelli/millimanintelli.csv\n"
     ]
    }
   ],
   "source": [
    "# read list the files in S3 subdirectory using Python\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "## Bucket to use\n",
    "bucket = s3.Bucket('Bucket')  # which Bucket\n",
    "\n",
    "## List objects within a given prefix\n",
    "for obj in bucket.objects.filter(Delimiter='/', Prefix='file/'): # Prefix is which directory we need to find\n",
    "  print (obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import * # we can also use the spark to read the csv on our computer\n",
    "#df = sql_c.read.csv('/Users/katie.xia/Desktop/project-classifier/nwdata.csv', inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the spark to connect s3 and read s3's file (the set up)\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import configparser\n",
    "\n",
    "access_id = \"********\"\n",
    "access_key =\"********\"\n",
    "# set spark\n",
    "spark = SparkSession.builder.master('local').appName(\"test\").getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "hadoop_conf=sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3n.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "hadoop_conf.set(\"fs.s3n.awsAccessKeyId\", access_id)\n",
    "hadoop_conf.set(\"fs.s3n.awsSecretAccessKey\", access_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_song = spark.read.csv('s3a://sikkasparkdata/millimanintelli/millimanintelli.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use spark to read csv in s3\n",
    "millimanintelli=spark.read.csv('s3n://Bucket/file/file1.csv',inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(PersonID=42890708, FirstName='Belinda', LastName='Harper', DOB=datetime.datetime(1980, 5, 16, 0, 0), SSN='563613038', Gender='F', ZipCode='93505'),\n",
       " Row(PersonID=42890709, FirstName='Alberto', LastName='Torres', DOB=datetime.datetime(1964, 12, 12, 0, 0), SSN='611626678', Gender='M', ZipCode='90638'),\n",
       " Row(PersonID=42890710, FirstName='Kiran', LastName='Mann', DOB=datetime.datetime(1970, 8, 29, 0, 0), SSN='         ', Gender='F', ZipCode='95337'),\n",
       " Row(PersonID=42890712, FirstName='Tazari', LastName='Woodward', DOB=datetime.datetime(1998, 11, 5, 0, 0), SSN='607114739', Gender='F', ZipCode='91764'),\n",
       " Row(PersonID=42890713, FirstName='Donna', LastName='Garvey', DOB=datetime.datetime(1947, 3, 6, 0, 0), SSN='523669738', Gender='F', ZipCode='80014')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "millimanintelli.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the view of sql\n",
    "millimanintelli.createOrReplaceTempView(\"table1\")\n",
    "#df2 = spark.sql(\"SELECT * from millimanintell limit 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------+-------------------+---------+------+-------+\n",
      "|PersonID|FirstName|LastName|                DOB|      SSN|Gender|ZipCode|\n",
      "+--------+---------+--------+-------------------+---------+------+-------+\n",
      "|42890708|  Belinda|  Harper|1980-05-16 00:00:00|563613038|     F|  93505|\n",
      "|42890709|  Alberto|  Torres|1964-12-12 00:00:00|611626678|     M|  90638|\n",
      "|42890710|    Kiran|    Mann|1970-08-29 00:00:00|         |     F|  95337|\n",
      "|42890712|   Tazari|Woodward|1998-11-05 00:00:00|607114739|     F|  91764|\n",
      "|42890713|    Donna|  Garvey|1947-03-06 00:00:00|523669738|     F|  80014|\n",
      "+--------+---------+--------+-------------------+---------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use sql in spark\n",
    "df1 = spark.sql(\"SELECT * from table1 limit 5\")\n",
    "df1.show()  # millimanintelli in sql (table 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use spark to read text file in spark\n",
    "from pyspark.sql.types import *\n",
    "schema_notetext = StructType([\n",
    "   StructField(\"PatID\", StringType()),\n",
    "   StructField(\"Fname\", StringType()),\n",
    "   StructField(\"Lname\", StringType()),\n",
    "   StructField(\"Bdate\", DateType()),\n",
    "   StructField(\"Add1\", StringType()),\n",
    "   StructField(\"Add2\", StringType()),\n",
    "   StructField(\"City\", StringType()),\n",
    "   StructField(\"State\", StringType()),\n",
    "   StructField(\"Zip\", StringType()),\n",
    "   StructField(\"SSN_second\", StringType()),\n",
    "   StructField(\"NoteType\", StringType()),\n",
    "   StructField(\"NoteName\", StringType()),\n",
    "   StructField(\"NoteValue\", StringType()),\n",
    "   StructField(\"DateCreated\", DateType()),\n",
    "   StructField(\"DateUpdated\", DateType()),\n",
    "   StructField(\"Custid\", IntegerType()),\n",
    "   StructField(\"PracticeID\", StringType()),\n",
    "   StructField(\"MasterID\", StringType())\n",
    "   ])\n",
    "df_notetext = spark.read \\\n",
    "  .format(\"com.databricks.spark.csv\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"delimiter\",\",\") \\\n",
    "  .option('ignoreLeadingWhiteSpace',\"true\") \\\n",
    "  .option('ignoreTrailingWhiteSpace', \"true\") \\\n",
    "  .option(\"treatEmptyValuesAsNulls\",\"true\") \\\n",
    "  .option(\"treatParseExceptionAsNull\",\"true\") \\\n",
    "  .option(\"insertNullOnErrors\",\"true\") \\\n",
    "  .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "  .schema(schema_notetext) \\\n",
    "  .load(\"s3n://sikkasparkdata/dental/patientmedicalnotes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table in spark sql and use sql in spark\n",
    "df_notetext.createOrReplaceTempView(\"table2\")\n",
    "df2 = spark.sql(\"SELECT * from table2 limit 10\") # df2 is table2 is df_notetext in sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
