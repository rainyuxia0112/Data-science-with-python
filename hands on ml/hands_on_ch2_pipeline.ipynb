{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the exploratiry data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get the data\n",
    "    # data structure \n",
    "            ## data.head(); data.info(), data.col.value_counts(); 查看nan个数。\n",
    "            ## 做完基础查看后应直接split data\n",
    "            ## vasualization： correlation（pearson）， scatter\n",
    "            ## data cleaning: fillna, imputer, normalization\n",
    "    # doing datapipe and tune the model\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, train_y, test_x, test_y = train_test_split(data, test_size = 0.3) \n",
    "# note: 该方法仅适用于big data，，若数据量不大，要保持representative 则使用stratified sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified sampling\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2) # n_splits 是将数据集分成 train/test pair的组数\n",
    "for train_index, test_index in split.split(X, y):  \n",
    "   print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "   X_train, X_test = X[train_index], X[test_index]\n",
    "   y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "# 有几组代表loop循环几次\n",
    "# if 只有一组的话： list(split.split(X, y)) 就获得了分组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [5 2 6 4 1 3] TEST: [7 0]\n",
      "TRAIN: [4 3 5 2 7 1] TEST: [6 0]\n",
      "TRAIN: [7 1 6 2 0 4] TEST: [5 3]\n",
      "TRAIN: [3 6 4 7 0 5] TEST: [1 2]\n",
      "TRAIN: [3 4 1 7 2 0] TEST: [6 5]\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4],\n",
    "              [1, 2],[3, 4], [1, 2], [3, 4]])#训练数据集8*2\n",
    "y = np.array([0, 0, 1, 1,0,0,1,1])#类别数据集8*1\n",
    "\n",
    "ss=StratifiedShuffleSplit(n_splits=5,test_size=0.25,train_size=0.75,random_state=0)#分成5组，测试比例为0.25，训练比例是0.75\n",
    "\n",
    "for train_index, test_index in ss.split(X, y):  #有5组，每一组有train， test\n",
    "   print(\"TRAIN:\", train_index, \"TEST:\", test_index)#获得索引值\n",
    "   X_train, X_test = X[train_index], X[test_index]#训练集对应的值\n",
    "   y_train, y_test = y[train_index], y[test_index]#类别集对应的值\n",
    "\n",
    "# # 可以用来cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  vasualization\n",
    "    ## scatter plot \n",
    "df.plot(kind = 'scatter', x = '', y= '', alpha = 0.1) # alpha: high density data points 就会变深\n",
    "# 一张图上有时候可以表达两个指标：p57\n",
    "housing.plot(kind = 'scatter', x = 'longitude', y = 'latitude', alpha = 0.4, \n",
    "            s = housing.population/100, label = 'population', figsize = (10,7),\n",
    "            c = 'price', cmap = plt.get_cmap('jet'), colorbar = True)\n",
    "plt.legend()\n",
    "\n",
    "# s: 每一个点的半径大小和人口数量有关， label：是人口（右上角的）， c:颜色和房价有关， cmap： 用什么颜色图谱表示, colorbar\n",
    "    # 可以看房价和人口多少，地点有没有关系\n",
    "\n",
    "\n",
    "# corelation\n",
    "    ## 第一种，直接算correlation 值： df.corr(): 会得到一个matrix\n",
    "pearson_matrix = df.corr()\n",
    "    ## 第二种，画图\n",
    "from pandas.plotting import scatter_matrix\n",
    "attributes = ['', '', '']\n",
    "scatter_matrix(df[attributes], figsize = (12,8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## impute package\n",
    "https://scikit-learn.org/stable/modules/impute.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "from sklearn.pipeline import FeatureUnion, make_pipeline, Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 只有连续变量， indicator必须要用 FeatureUnion\n",
    "transformer = FeatureUnion(transformer_list=\n",
    "                           [('features', SimpleImputer(strategy='mean')),('indicators', MissingIndicator())])\n",
    "\n",
    "\n",
    "# fit the model\n",
    "clf = Pipeline([('trans',transformer), ('class', DecisionTreeClassifier())])\n",
    "## 也可以写成：\n",
    "clf = make_pipeline(transformer, DecisionTreeClassifier())\n",
    "clf = clf.fit(X_train, y_train)\n",
    "results = clf.predict(X_test)\n",
    "results.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalization\n",
    "    ## minmaxscalar:(feature_range)\n",
    "    ## standardscaler: standardazation: does not bound values to a specific range, less affected by outliers\n",
    "https://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n",
    "    \n",
    "## 到底哪些模型需要normalization呢？\n",
    "\n",
    "概率模型（树形模型）不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、RF。而像Adaboost、SVM、LR、Knn、KMeans之类的最优化问题就需要归一化。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对连续变量和离散变量一起写数据预处理pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "num_attribs = [一些连续变量]\n",
    "cat_attribs = [分类变量]\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy = 'median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_attribs),\n",
    "    ('cat', cat_pipeline, cat_attribs)\n",
    "    \n",
    "])\n",
    "\n",
    "new = full_pipeline.fit_transform(old) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对连续变量和离散变量一起写模型pipeline\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.790\n"
     ]
    }
   ],
   "source": [
    "# Author: Pedro Morales <part.morales@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load data from https://www.openml.org/d/40945\n",
    "X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
    "\n",
    "# Alternatively X and y can be obtained directly from the frame attribute:\n",
    "# X = titanic.frame.drop('survived', axis=1)\n",
    "# y = titanic.frame['survived']\n",
    "\n",
    "# We will train our classifier with the following features:\n",
    "# Numeric Features:\n",
    "# - age: float.\n",
    "# - fare: float.\n",
    "# Categorical Features:\n",
    "# - embarked: categories encoded as strings {'C', 'S', 'Q'}.\n",
    "# - sex: categories encoded as strings {'female', 'male'}.\n",
    "# - pclass: ordinal integers {1, 2, 3}.\n",
    "\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "numeric_features = ['age', 'fare'] # 连续\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['embarked', 'sex', 'pclass'] #分类\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)]) # 合在一起，de\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine tune the model\n",
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best logistic regression from grid search: 0.790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '1', '1', '0', '1', '0', '1', '0', '0', '0', '0', '1', '0', '1',\n",
       "       '0', '0', '0', '1', '1', '1', '1', '0', '1', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '1', '0', '0', '1', '0', '0', '1', '0',\n",
       "       '0', '1', '0', '0', '1', '0', '1', '1', '0', '0', '1', '0', '1',\n",
       "       '1', '0', '0', '0', '1', '1', '0', '0', '0', '1', '1', '0', '1',\n",
       "       '1', '0', '0', '1', '0', '0', '1', '1', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '1', '0', '1', '0', '0', '0', '1',\n",
       "       '0', '0', '1', '0', '0', '0', '0', '1', '0', '0', '1', '0', '0',\n",
       "       '0', '0', '1', '0', '0', '1', '1', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '1', '0', '1', '0', '0', '0', '1', '0', '1', '0', '1', '0',\n",
       "       '0', '0', '0', '1', '0', '1', '0', '1', '0', '0', '1', '0', '0',\n",
       "       '1', '1', '0', '0', '0', '0', '0', '1', '0', '1', '0', '0', '0',\n",
       "       '0', '0', '0', '1', '0', '1', '0', '0', '1', '1', '1', '1', '0',\n",
       "       '1', '0', '1', '1', '1', '0', '0', '0', '0', '0', '1', '0', '0',\n",
       "       '1', '0', '1', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0',\n",
       "       '0', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '0', '1',\n",
       "       '1', '1', '1', '1', '0', '1', '1', '1', '1', '1', '0', '0', '1',\n",
       "       '0', '0', '1', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0',\n",
       "       '0', '1', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0',\n",
       "       '0', '0'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author: Pedro Morales <part.morales@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load data from https://www.openml.org/d/40945\n",
    "X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
    "\n",
    "# Alternatively X and y can be obtained directly from the frame attribute:\n",
    "# X = titanic.frame.drop('survived', axis=1)\n",
    "# y = titanic.frame['survived']\n",
    "\n",
    "# We will train our classifier with the following features:\n",
    "# Numeric Features:\n",
    "# - age: float.\n",
    "# - fare: float.\n",
    "# Categorical Features:\n",
    "# - embarked: categories encoded as strings {'C', 'S', 'Q'}.\n",
    "# - sex: categories encoded as strings {'female', 'male'}.\n",
    "# - pclass: ordinal integers {1, 2, 3}.\n",
    "\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "numeric_features = ['age', 'fare'] \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy=param_grid['preprocessor__num__imputer__strategy'])),   # 要超参数放进去\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['embarked', 'sex', 'pclass'] #分类\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)]) # 合在一起，de\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])\n",
    "\n",
    "############################################################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "    #'classifier__C': [0.1, 1.0, 10, 100],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=10) # clf 是上面的pipeline\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.cv_results_ # 一个dict，转成df看，找到tune完的结果\n",
    "print((\"best logistic regression from grid search: %.3f\"\n",
    "       % grid_search.score(X_test, y_test)))\n",
    "\n",
    "# 得到最优参数后，重新train？不需要直接给结果： \n",
    "    ## predict(X) Call predict on the estimator with the best found parameters.\n",
    "grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python37/lib/python3.7/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 2 is smaller than n_iter=10. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best logistic regression from grid search: 0.794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "       '0', '0', '1', '1', '0', '0', '1', '1', '1', '1', '1', '0', '1',\n",
       "       '0', '1', '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '1',\n",
       "       '1', '0', '1', '1', '0', '0', '0', '1', '0', '0', '0', '0', '0',\n",
       "       '0', '1', '0', '0', '0', '0', '0', '1', '0', '0', '1', '0', '1',\n",
       "       '1', '1', '1', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1',\n",
       "       '1', '0', '0', '1', '0', '1', '0', '0', '0', '1', '0', '0', '0',\n",
       "       '0', '0', '1', '0', '0', '0', '0', '0', '0', '1', '1', '0', '0',\n",
       "       '0', '1', '0', '1', '0', '1', '0', '0', '1', '1', '1', '0', '1',\n",
       "       '0', '1', '1', '1', '1', '0', '0', '0', '1', '0', '0', '1', '0',\n",
       "       '0', '0', '0', '0', '1', '0', '0', '1', '1', '1', '1', '0', '1',\n",
       "       '1', '1', '0', '1', '1', '0', '0', '0', '0', '1', '0', '0', '0',\n",
       "       '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '1', '0', '0',\n",
       "       '1', '0', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '1',\n",
       "       '0', '0', '0', '1', '0', '0', '0', '0', '1', '0', '1', '1', '0',\n",
       "       '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '1',\n",
       "       '0', '0', '0', '0', '0', '0', '1', '0', '1', '0', '0', '0', '1',\n",
       "       '0', '0', '1', '1', '0', '1', '0', '0', '0', '0', '1', '1', '0',\n",
       "       '0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '1', '0', '1',\n",
       "       '1', '0', '0', '1', '0', '1', '0', '0', '0', '1', '0', '1', '0',\n",
       "       '0', '0'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "    #'classifier__C': [0.1, 1.0, 10, 100],\n",
    "}\n",
    "\n",
    "grid_search = RandomizedSearchCV(clf, param_grid, cv=10) # clf 是上面的pipeline\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.cv_results_ # 一个dict，转成df看，找到tune完的结果\n",
    "print((\"best logistic regression from grid search: %.3f\"\n",
    "       % grid_search.score(X_test, y_test)))\n",
    "\n",
    "# 得到最优参数后，重新train？不需要直接给结果： \n",
    "    ## predict(X) Call predict on the estimator with the best found parameters.\n",
    "grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
